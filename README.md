# Predict virus and host interactions based on pre-trained transformer model and attention-based multiple instances learning
Dan Liu, Fran Young, Ke Yuan*, David L Robertson*
### Abstract:
Detecting virus-host interactions is essential for us to understand interaction mechanisms between virus and host and explore clues regarding diseases caused by viruses. However, host information of the majority of viruses are unknown. The virus mimics the host's patterns for escaping the immune response, viruses might contain features that are related with virus-host interactions. we introduce EvoMIL, a deep learning method that predicts virus-host association at the species level from viral sequence only. The method combines a pre-trained large protein language model and attention-based multiple instance learning (MIL) to allow protein-orientated predictions. Our results show that protein embeddings capture stronger predictive signals than traditional handcrafted features, including amino acids and DNA k-mers, and physio-chemical properties. Furthermore, EvoMIL estimates the importance of single proteins in the prediction and maps them to an embedding landscape of all viral proteins, where proteins with similar functions are distinctly clustered together.

## PreTLM-MIL
###  The flowchart of PreTLM-MIL
First, protein sequences of viruses and virus-host interactions can be collected from the VHDB (https://www.genome.jp/virushostdb/). For each host, we get the same number of positive viruses and negative viruses, and then representative vectors of viral protein sequences are obtained by the pre-trained transformer model ESM-1b https://github.com/facebookresearch/esm. There is a host label for a set of protein sequences of each virus, attention-based MIL is applied to predict the host label for each virus, and calculate instance weights for each protein of viruses. Finally, probability between the virus and the given host is obtained to check if a virus is associated with the host.

![flowchart](https://github.com/liudan111/EvoMIL/blob/260741ea0d981e060b945be0e6a542515e2bd588/Figures/flowchart.pdf)

### Requirements:
    Python3
    PyTorch ==1.7.1 (with CUDA)
    torchvision == 0.8.2
    sklearn==0.23.2
    numpy==1.21.5

### Processing steps of PreTLM-MIL
1.  Collect viruses which are linked with hosts from VHDB https://www.genome.jp/virushostdb/, and construct the balance datasets for each host.

2.  Pre-trained transformer model ESM-1b (https://github.com/facebookresearch/esm) to calculate embeddings of each protein of viruses.

3.  Train binary and multi-class classification models by attention-based MIL.


### Data
1.  virushostdb_update.csv
    This table includes all links between viruses and hosts from VHDB datasets.
    
2. final_specise_sort_30_pro.csv 
    It is the table of prokaryotic hosts: hostname and number of viruses related to the host.

3. final_specise_sort_45_euk.csv
    It is the table of eukaryotic hosts: hostname and number of viruses related to the host.
    
4. example/5fold_cv 
    There are training and validation datasets generated by 5-folod cross validation, and a test dataset.

    
### Codes for training and testing on dataset (Binary classification).
1. Model/model_esm1b.py
    It is the attention-based MIL model for binary classification.

2. Model/train.py
    Train the model by 5-fold cross validation.

3. Model/test.py
    Test 5 trained models on the test data, then we can choose the model by the best AUC or Accuracy.
    
4. mil_pytorch are mil package from 

### Codes for training and testing on dataset (Multi-class classification).
1. Model_mc/attention_mc.py
    It is the attention-based MIL model for multi-class classification.

2. Model/train_mc.py
    Train the model by 5-fold cross validation.

3. Model/test_mc.py
    Test 5 trained models on the test data, then we can choose the model by the best AUC or Accuracy.


### Trained models on prokaryotic hosts
1. Trained model_pro/best_model_acc 
    Based on 5-fold cross validation, we trained our model on 21 prokaryotic hosts separately, then test five models on test datasets to obtain a model with the best Accuracy for each prokaryotic host.

2. Trained model_pro/best_model_auc 
    Based on 5-fold cross validation, we trained our model on 21 prokaryotic hosts separately, then test five models on test datasets to obtain a model with the best AUC for each prokaryotic host.

3. Trained model_pro/final_model
    Here are trained final models for each prokaryotic host, they are trained on all datasets we collect from VHDB.

### Trained models on eukaryotic hosts
1. Trained model_euk/best_model_acc 
    Based on 5-fold cross validation, we trained our model on 12 eukaryotic hosts separately, then test five models on test datasets to obtain a model with the best Accuracy for each eukaryotic host.

2. Trained model_euk/best_model_auc 
    Based on 5-fold cross validation, we trained our model on 12 eukaryotic hosts separately, then test five models on test datasets to obtain a model with the best AUC for each eukaryotic host.

3. Trained model_euk/final_model
    Here are trained final models for each eukaryotic host, they are trained on all datasets we collect from VHDB.
