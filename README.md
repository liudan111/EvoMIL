# Predict virus and host interactions based on pre-trained transformer model and attention-based multiple instances learning
Dan Liu, Fran Young, Ke Yuan*, David L Robertson*
### Abstract:
Detecting virus-host interactions is essential for us to understand interaction mechanisms between virus and host and explore clues regarding diseases caused by viruses. However, host information of the majority of viruses are unknown. The virus mimics the host's patterns for escaping the immune response, viruses might contain features that are related with virus-host interactions. We introduce PreTLM-MIL, extracting protein features by a pre-trained transformer model, then applying attention-based multiple instance learning (MIL) to predict hosts and calculate weight of each protein, which can be used to explain key proteins associated with hosts. 

## PreTLM-MIL
###  The flowchart of PreTLM-MIL
First, protein sequences of viruses and virus-host interactions can be collected from the VHDB (https://www.genome.jp/virushostdb/). For each host, we get the same number of positive viruses and negative viruses, and then representative vectors of viral protein sequences are obtained by the pre-trained transformer model ESM-1b https://github.com/facebookresearch/esm. There is a host label for a set of protein sequences of each virus, attention-based MIL is applied to predict the host label for each virus, and calculate instance weights for each protein of viruses. Finally, probability between the virus and the given host is obtained to check if a virus is associated with the host.

![flowchart](https://user-images.githubusercontent.com/6703505/191104200-99f5d421-4a96-4201-ae68-2bee49b060d2.png)


### Requirements:
    Python3
    PyTorch ==1.7.1 (with CUDA)
    torchvision == 0.8.2
    sklearn==0.23.2
    numpy==1.21.5

### Processing steps of PreTLM-MIL
1.  Collect viruses which are linked with hosts from VHDB https://www.genome.jp/virushostdb/, and construct the balance datasets for each host.

2.  Pre-trained transformer model ESM-1b (https://github.com/facebookresearch/esm) to calculate embeddings of each protein of viruses.

3.  Train binary and multi-class classification models by attention-based MIL 
### Data
1. example/5fold_cv 

    There are five training and validation datasets generated by 5-folod cross validation, and a test dataset.

2. final_specise_sort_30_pro.csv 

   It is the table 

3. final_specise_sort_45_euk.csv

    It is the table 

4.  virushostdb_update.csv

    This table includes all links between viruses and hosts from VHDB datasets.
    
### Codes for training and testing on dataset (Binary classification).
1. Model/model_esm1b.py

    It is the attention-based MIL model for binary classification.

2. Model/train.py

    Train the model by 5-fold cross validation.

3. Model/test.py

    Test 5 trained models on the test data, then we can choose the model by the best AUC or Accuracy.
    
### Codes for training and testing on dataset (Multi-class classification).
1. Model_mc/attention_mc.py

    It is the attention-based MIL model for multi-class classification.

2. Model/train_mc.py

    Train the model by 5-fold cross validation.

3. Model/test_mc.py

    Test 5 trained models on the test data, then we can choose the model by the best AUC or Accuracy.


### Trained models on prokaryotic hosts

1. Trained model_pro/best_model_acc 

    This folder includes trained models for 21 prokaryotic hosts

2. Trained model_pro/best_model_auc 


3. Trained model_pro/final_model

### Trained models on eukaryotic hosts

1. Trained model_euk/best_model_acc 


2. Trained model_euk/best_model_auc 


3. Trained model_euk/final_model
